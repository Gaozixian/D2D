{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa8bb7b088aa2010",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 主函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cefe77019a0bfca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-26T14:39:48.830912300Z",
     "start_time": "2026-02-26T14:39:45.512765700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成！归一化参数已保存至 scaler_params.json，序列化数据已保存至 global_vehicle_data_history_cols.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "双流端到端自动驾驶网络 (Dual-Stream End-to-End Driving Model)\n",
    "- 视觉流: ResNet_CBAM + Transformer (处理 t-2, t-1, t 三帧图像)\n",
    "- 状态流: LSTM (处理过去 N 帧的 [速度, 加速度, 转角])\n",
    "- 融合策略: Output = MLP(Concat(Visual, LSTM)) + LSTM\n",
    "- 预测输出: 当前时刻所需的 [加速度, 转角]\n",
    "\"\"\"\n",
    "\n",
    "# 还原数据\n",
    "# target_names = ['acceleration_x', 'acceleration_y', 'acceleration_z', 'steer']\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 优化前版本，已经弃用",
   "id": "b3bf28da0388b12b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 导入你自己写的各个模块\n",
    "from dataset_create import prepare_dataset_and_scaler, ProcessedDrivingDataset, inverse_transform\n",
    "from model import DualStreamDrivingModel\n",
    "\n",
    "def main():\n",
    "    # 1. 配置文件路径\n",
    "    input_csv = 'global_vehicle_data.csv'\n",
    "    output_csv = 'global_vehicle_data_history_cols.csv'\n",
    "    scaler_json = 'scaler_params.json'\n",
    "    target_names = ['acceleration_x', 'acceleration_y', 'acceleration_z', 'steer']\n",
    "\n",
    "    # 2. 如果之前没有预处理数据，则调用预处理（这里可以按需注释）\n",
    "    print(\">>> 开始检查并预处理数据...\")\n",
    "    scaler_params = prepare_dataset_and_scaler(\n",
    "        input_file=input_csv,\n",
    "        output_csv=output_csv,\n",
    "        scaler_json_path=scaler_json,\n",
    "        seq_length=9\n",
    "    )\n",
    "    print(\">>> 预处理完成！\\n\")\n",
    "\n",
    "    # 3. 加载数据集\n",
    "    dataset = ProcessedDrivingDataset(csv_file=output_csv)\n",
    "    dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "\n",
    "    # 4. 初始化端到端模型和损失函数\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = DualStreamDrivingModel().to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    print(f\">>> 模型初始化成功，运行在 {device} 上，开始测试 Forward Pass...\\n\")\n",
    "\n",
    "    # 5. 模拟训练循环\n",
    "    for batch_idx, (images, state_seq, target) in enumerate(dataloader):\n",
    "        # 数据移至对应设备\n",
    "        img_t_minus_2, img_t_minus_1, img_t = [img.to(device) for img in images]\n",
    "        state_seq = state_seq.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # 模型前向传播\n",
    "        predictions = model(img_t_minus_2, img_t_minus_1, img_t, state_seq)\n",
    "        loss = criterion(predictions, target)\n",
    "\n",
    "        # 打印调试信息\n",
    "        print(f\"--- Batch {batch_idx} ---\")\n",
    "        print(f\"图像尺寸: {img_t.shape}\")\n",
    "        print(f\"LSTM 输入尺寸: {state_seq.shape} -> (Batch=4, Seq=8, Features=10)\")\n",
    "        print(f\"预测输出尺寸: {predictions.shape}\")\n",
    "        print(f\"当前 Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # 测试反归一化还原 (取 Batch 中的第 0 个样本)\n",
    "        pred_numpy = predictions[0].detach().cpu().numpy()\n",
    "        real_values = inverse_transform(pred_numpy, target_names, scaler_params)\n",
    "        print(f\"\\n模型下发的真实控制指令 [acc_x, acc_y, acc_z, steer]: \\n{real_values}\\n\")\n",
    "\n",
    "        # 只跑一个 Batch 验证\n",
    "        break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "2b34a987c0508a40"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 训练代码",
   "id": "780124abb9fced9e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-27T08:51:23.175834400Z",
     "start_time": "2026-02-27T08:51:13.259851100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from dataset_create import prepare_dataset_and_scaler, ProcessedDrivingDataset, inverse_transform\n",
    "from model import DualStreamDrivingModel\n",
    "from datetime import datetime\n",
    "\n",
    "def train():\n",
    "    # 配置文件路径\n",
    "    input_csv = 'global_vehicle_data.csv'\n",
    "    output_csv = 'global_vehicle_data_history_cols.csv'\n",
    "    scaler_json = 'scaler_params.json'\n",
    "    target_names = ['acceleration_x', 'acceleration_y', 'acceleration_z', 'steer']\n",
    "\n",
    "    # 数据预处理\n",
    "    print(\">>> 开始检查并预处理数据...\")\n",
    "    scaler_params = prepare_dataset_and_scaler(\n",
    "        input_file=input_csv,\n",
    "        output_csv=output_csv,\n",
    "        scaler_json_path=scaler_json,\n",
    "        seq_length=9\n",
    "    )\n",
    "    print(\">>> 预处理完成！\\n\")\n",
    "\n",
    "    # 加载数据集\n",
    "    dataset = ProcessedDrivingDataset(csv_file=output_csv)\n",
    "    dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "\n",
    "    # 初始化模型、损失函数、优化器\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = DualStreamDrivingModel().to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "    scheduler = StepLR(optimizer, step_size=10, gamma=0.9)\n",
    "\n",
    "    # 日志文件\n",
    "    log_dir = \"logs\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    log_file = os.path.join(log_dir, f\"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\")\n",
    "\n",
    "    # 训练循环\n",
    "    num_epochs = 20\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for batch_idx, (images, state_seq, target) in enumerate(dataloader):\n",
    "            # 数据移至设备\n",
    "            img_t_minus_2, img_t_minus_1, img_t = [img.to(device) for img in images]\n",
    "            state_seq = state_seq.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # 前向传播\n",
    "            predictions = model(img_t_minus_2, img_t_minus_1, img_t, state_seq)\n",
    "            loss = criterion(predictions, target)\n",
    "\n",
    "            # 反向传播与优化\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # 打印当前 Batch 的损失\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # 学习率调度\n",
    "        scheduler.step()\n",
    "\n",
    "        # 记录日志\n",
    "        with open(log_file, \"a\") as f:\n",
    "            f.write(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(dataloader):.4f}\\n\")\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] 完成，平均 Loss: {epoch_loss/len(dataloader):.4f}\")\n",
    "\n",
    "    # 保存模型\n",
    "    model_path = os.path.join(log_dir, \"dual_stream_model.pth\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"模型已保存至 {model_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ],
   "id": "5b7a3f5ddd50ff5a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 开始检查并预处理数据...\n",
      ">>> 预处理完成！\n",
      "\n"
     ]
    },
    {
     "ename": "UnidentifiedImageError",
     "evalue": "cannot identify image file 'carla_data_collect/20260130_164858/images/front/237_966510_front.png'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mUnidentifiedImageError\u001B[39m                    Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 86\u001B[39m\n\u001B[32m     83\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m模型已保存至 \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     85\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[34m__name__\u001B[39m == \u001B[33m\"\u001B[39m\u001B[33m__main__\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m---> \u001B[39m\u001B[32m86\u001B[39m     \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 51\u001B[39m, in \u001B[36mtrain\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m     48\u001B[39m model.train()\n\u001B[32m     49\u001B[39m epoch_loss = \u001B[32m0.0\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m51\u001B[39m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate_seq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdataloader\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m     52\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# 数据移至设备\u001B[39;49;00m\n\u001B[32m     53\u001B[39m \u001B[43m    \u001B[49m\u001B[43mimg_t_minus_2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimg_t_minus_1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimg_t\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mimg\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mimages\u001B[49m\u001B[43m]\u001B[49m\n\u001B[32m     54\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstate_seq\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate_seq\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\anaconda\\envs\\gzx\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:732\u001B[39m, in \u001B[36m_BaseDataLoaderIter.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    729\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    730\u001B[39m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[32m    731\u001B[39m     \u001B[38;5;28mself\u001B[39m._reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m732\u001B[39m data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    733\u001B[39m \u001B[38;5;28mself\u001B[39m._num_yielded += \u001B[32m1\u001B[39m\n\u001B[32m    734\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    735\u001B[39m     \u001B[38;5;28mself\u001B[39m._dataset_kind == _DatasetKind.Iterable\n\u001B[32m    736\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    737\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._num_yielded > \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called\n\u001B[32m    738\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\anaconda\\envs\\gzx\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:788\u001B[39m, in \u001B[36m_SingleProcessDataLoaderIter._next_data\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    786\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    787\u001B[39m     index = \u001B[38;5;28mself\u001B[39m._next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m788\u001B[39m     data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m    789\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._pin_memory:\n\u001B[32m    790\u001B[39m         data = _utils.pin_memory.pin_memory(data, \u001B[38;5;28mself\u001B[39m._pin_memory_device)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\anaconda\\envs\\gzx\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001B[39m, in \u001B[36m_MapDatasetFetcher.fetch\u001B[39m\u001B[34m(self, possibly_batched_index)\u001B[39m\n\u001B[32m     50\u001B[39m         data = \u001B[38;5;28mself\u001B[39m.dataset.__getitems__(possibly_batched_index)\n\u001B[32m     51\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m52\u001B[39m         data = \u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpossibly_batched_index\u001B[49m\u001B[43m]\u001B[49m\n\u001B[32m     53\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     54\u001B[39m     data = \u001B[38;5;28mself\u001B[39m.dataset[possibly_batched_index]\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\anaconda\\envs\\gzx\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001B[39m, in \u001B[36m<listcomp>\u001B[39m\u001B[34m(.0)\u001B[39m\n\u001B[32m     50\u001B[39m         data = \u001B[38;5;28mself\u001B[39m.dataset.__getitems__(possibly_batched_index)\n\u001B[32m     51\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m52\u001B[39m         data = [\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[32m     53\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     54\u001B[39m     data = \u001B[38;5;28mself\u001B[39m.dataset[possibly_batched_index]\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Laboratory files\\code_project\\D2D\\main\\dataset_create.py:120\u001B[39m, in \u001B[36mProcessedDrivingDataset.__getitem__\u001B[39m\u001B[34m(self, idx)\u001B[39m\n\u001B[32m    118\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m path \u001B[38;5;129;01min\u001B[39;00m img_paths:\n\u001B[32m    119\u001B[39m     full_path = os.path.join(\u001B[38;5;28mself\u001B[39m.root_dir, path) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.root_dir \u001B[38;5;28;01melse\u001B[39;00m path\n\u001B[32m--> \u001B[39m\u001B[32m120\u001B[39m     img = \u001B[43mImage\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfull_path\u001B[49m\u001B[43m)\u001B[49m.convert(\u001B[33m'\u001B[39m\u001B[33mRGB\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m    121\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.transform:\n\u001B[32m    122\u001B[39m         img = \u001B[38;5;28mself\u001B[39m.transform(img)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\anaconda\\envs\\gzx\\Lib\\site-packages\\PIL\\Image.py:3560\u001B[39m, in \u001B[36mopen\u001B[39m\u001B[34m(fp, mode, formats)\u001B[39m\n\u001B[32m   3558\u001B[39m     warnings.warn(message)\n\u001B[32m   3559\u001B[39m msg = \u001B[33m\"\u001B[39m\u001B[33mcannot identify image file \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[33m\"\u001B[39m % (filename \u001B[38;5;28;01mif\u001B[39;00m filename \u001B[38;5;28;01melse\u001B[39;00m fp)\n\u001B[32m-> \u001B[39m\u001B[32m3560\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m UnidentifiedImageError(msg)\n",
      "\u001B[31mUnidentifiedImageError\u001B[39m: cannot identify image file 'carla_data_collect/20260130_164858/images/front/237_966510_front.png'"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
