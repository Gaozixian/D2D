{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa8bb7b088aa2010",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 完整的框架"
   ]
  },
  {
   "cell_type": "code",
   "id": "6cefe77019a0bfca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-26T14:39:48.830912300Z",
     "start_time": "2026-02-26T14:39:45.512765700Z"
    }
   },
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "双流端到端自动驾驶网络 (Dual-Stream End-to-End Driving Model)\n",
    "- 视觉流: ResNet_CBAM + Transformer (处理 t-2, t-1, t 三帧图像)\n",
    "- 状态流: LSTM (处理过去 N 帧的 [速度, 加速度, 转角])\n",
    "- 融合策略: Output = MLP(Concat(Visual, LSTM)) + LSTM\n",
    "- 预测输出: 当前时刻所需的 [加速度, 转角]\n",
    "\"\"\"\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import json\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 自建函数\n",
    "from model import prepare_dataset_and_scaler, inverse_transform\n",
    "\n",
    "\n",
    "# 执行处理\n",
    "scaler_params = prepare_dataset_and_scaler(\n",
    "    input_file='csv.csv',\n",
    "    output_csv='global_vehicle_data_history_cols.csv',\n",
    "    scaler_json_path='scaler_params.json',\n",
    "    seq_length=9\n",
    ")\n",
    "print(\"处理完成！归一化参数已保存至 scaler_params.json，序列化数据已保存至 global_vehicle_data_history_cols.csv\")\n",
    "\n",
    "# 还原数据\n",
    "# target_names = ['acceleration_x', 'acceleration_y', 'acceleration_z', 'steer']\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成！归一化参数已保存至 scaler_params.json，序列化数据已保存至 global_vehicle_data_history_cols.csv\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3690df7037a7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==================== 1. 适配新格式的 Dataset ====================\n",
    "\n",
    "class ProcessedDrivingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    专门解析带有 JSON history 列的端到端驾驶数据集\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_file, root_dir=\"\", transform=None):\n",
    "        self.data_df = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "        # 定义需要送入 LSTM 的 10 个数值特征列 (严格对应预处理阶段)\n",
    "        self.numeric_cols = [\n",
    "            'global_x_history', 'global_y_history', 'global_z_history',\n",
    "            'velocity_x_history', 'velocity_y_history', 'velocity_z_history',\n",
    "            'steer_history',\n",
    "            'acceleration_x_history', 'acceleration_y_history', 'acceleration_z_history'\n",
    "        ]\n",
    "\n",
    "        if transform is None:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data_df.iloc[idx]\n",
    "\n",
    "        # ---------------- A. 视觉数据 (取最后3帧) ----------------\n",
    "        front_images = json.loads(row['front_image_history'])\n",
    "        # 截取 t-2, t-1, t\n",
    "        img_paths = [front_images[-3], front_images[-2], front_images[-1]]\n",
    "\n",
    "        images = []\n",
    "        for path in img_paths:\n",
    "            full_path = os.path.join(self.root_dir, path) if self.root_dir else path\n",
    "            img = Image.open(full_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            images.append(img)\n",
    "\n",
    "        img_t_minus_2, img_t_minus_1, img_t = images\n",
    "\n",
    "        # ---------------- B. 状态历史数据 (N=9帧, Features=10) ----------------\n",
    "        # 构建形状为 (9, 10) 的矩阵\n",
    "        state_features = []\n",
    "        for col in self.numeric_cols:\n",
    "            # 解析 json list，长度为 9\n",
    "            val_list = json.loads(row[col])\n",
    "            state_features.append(val_list)\n",
    "\n",
    "        # state_features: 10 x 9 -> 转置为 9 x 10\n",
    "        state_seq = np.array(state_features, dtype=np.float32).T\n",
    "        state_seq_tensor = torch.tensor(state_seq)\n",
    "\n",
    "        # ---------------- C. 目标标签 (Targets=4) ----------------\n",
    "        # [accel_x, accel_y, accel_z, steer]\n",
    "        target_tensor = torch.tensor([\n",
    "            row['target_acceleration_x'],\n",
    "            row['target_acceleration_y'],\n",
    "            row['target_acceleration_z'],\n",
    "            row['target_steer']\n",
    "        ], dtype=torch.float32)\n",
    "\n",
    "        return (img_t_minus_2, img_t_minus_1, img_t), state_seq_tensor, target_tensor\n",
    "\n",
    "\n",
    "# ==================== 2. 模型核心组件 (与预处理结构对齐) ====================\n",
    "# (此处省略部分 ResNet 基础块定义以保持简洁，使用时请补全之前的 Bottleneck/ResNet 代码)\n",
    "\n",
    "class MotionLSTMEncoder(nn.Module):\n",
    "    \"\"\"LSTM 处理历史状态\"\"\"\n",
    "    # 核心修改：input_size 修改为 10\n",
    "    def __init__(self, input_size=10, hidden_size=512, num_layers=2):\n",
    "        super(MotionLSTMEncoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        self.ln = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"x shape: (Batch, seq_length=9, features=10)\"\"\"\n",
    "        output, (h_n, c_n) = self.lstm(x)\n",
    "        last_hidden = h_n[-1]\n",
    "        return self.ln(last_hidden)\n",
    "\n",
    "# 使用假 ResNet 占位（实际使用时替换回真实的 create_resnet_cbam）\n",
    "class MockResNet(nn.Module):\n",
    "    def get_feature_maps(self, x):\n",
    "        return torch.randn(x.size(0), 2048, 7, 7).to(x.device)\n",
    "def create_resnet_cbam(): return MockResNet()\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=500):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "class DualStreamDrivingModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 use_shared_weights=True,\n",
    "                 visual_d_model=512,\n",
    "                 lstm_hidden_size=512):\n",
    "        super(DualStreamDrivingModel, self).__init__()\n",
    "\n",
    "        self.use_shared_weights = use_shared_weights\n",
    "        self.visual_d_model = visual_d_model\n",
    "\n",
    "        # 1. 视觉流\n",
    "        self.resnet = create_resnet_cbam()\n",
    "        self.feature_proj = nn.Linear(2048, visual_d_model)\n",
    "        self.pos_encoder = PositionalEncoding(visual_d_model, max_len=500)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=visual_d_model, nhead=8, batch_first=True, dim_feedforward=1024, dropout=0.1\n",
    "        )\n",
    "        self.visual_transformer = nn.TransformerEncoder(encoder_layer, num_layers=3)\n",
    "\n",
    "        # 2. 状态流 (输入特征维度改为 10)\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.motion_lstm = MotionLSTMEncoder(\n",
    "            input_size=10,\n",
    "            hidden_size=lstm_hidden_size,\n",
    "            num_layers=2\n",
    "        )\n",
    "\n",
    "        # 3. 融合模块 MLP(Concat(Vis, LSTM))\n",
    "        fusion_input_dim = visual_d_model + lstm_hidden_size\n",
    "        self.fusion_mlp = nn.Sequential(\n",
    "            nn.Linear(fusion_input_dim, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, lstm_hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # 4. 预测头\n",
    "        # 核心修改：输出层改为 4 (对应 accel_x, accel_y, accel_z, steer)\n",
    "        self.control_head = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 4)\n",
    "        )\n",
    "\n",
    "    def extract_image_features(self, img_t_minus_2, img_t_minus_1, img_t):\n",
    "        f1 = self.resnet.get_feature_maps(img_t_minus_2)\n",
    "        f2 = self.resnet.get_feature_maps(img_t_minus_1)\n",
    "        f3 = self.resnet.get_feature_maps(img_t)\n",
    "        return f1, f2, f3\n",
    "\n",
    "    def forward(self, img_t_minus_2, img_t_minus_1, img_t, state_history):\n",
    "        batch_size = img_t.size(0)\n",
    "\n",
    "        # 视觉流\n",
    "        f1, f2, f3 = self.extract_image_features(img_t_minus_2, img_t_minus_1, img_t)\n",
    "        def flatten_and_project(f):\n",
    "            flat = f.view(batch_size, f.size(1), -1).transpose(1, 2)\n",
    "            return self.feature_proj(flat)\n",
    "        proj_1, proj_2, proj_3 = flatten_and_project(f1), flatten_and_project(f2), flatten_and_project(f3)\n",
    "\n",
    "        visual_seq = torch.cat([proj_1, proj_2, proj_3], dim=1)\n",
    "        visual_seq = self.pos_encoder(visual_seq)\n",
    "        trans_out = self.visual_transformer(visual_seq)\n",
    "        visual_vector = trans_out.mean(dim=1)\n",
    "\n",
    "        # 状态流\n",
    "        lstm_vector = self.motion_lstm(state_history)\n",
    "\n",
    "        # 融合与残差连接\n",
    "        combined_features = torch.cat([visual_vector, lstm_vector], dim=1)\n",
    "        mlp_output = self.fusion_mlp(combined_features)\n",
    "        fused_final = mlp_output + lstm_vector\n",
    "\n",
    "        # 预测\n",
    "        prediction = self.control_head(fused_final) # 输出尺寸 (Batch, 4)\n",
    "\n",
    "        return prediction\n",
    "\n",
    "# ==================== 3. 运行测试流程 ====================\n",
    "if __name__ == \"__main__\":\n",
    "    # 实例化 Dataset 和 DataLoader\n",
    "    dataset = ProcessedDrivingDataset(csv_file='global_vehicle_data_history_cols.csv')\n",
    "    dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "    # 初始化模型\n",
    "    model = DualStreamDrivingModel()\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # 抽取一个 Batch 测试\n",
    "    for batch_idx, (images, state_seq, target) in enumerate(dataloader):\n",
    "        img_t_minus_2, img_t_minus_1, img_t = images\n",
    "\n",
    "        # 前向传播\n",
    "        predictions = model(img_t_minus_2, img_t_minus_1, img_t, state_seq)\n",
    "        loss = criterion(predictions, target)\n",
    "\n",
    "        print(f\"--- Batch {batch_idx} ---\")\n",
    "        print(f\"图像输入尺寸 (单张): {img_t.shape}\")\n",
    "        print(f\"LSTM 输入尺寸: {state_seq.shape} -> (Batch, Seq=9, Features=10)\")\n",
    "        print(f\"预测值尺寸: {predictions.shape} -> [accel_x, accel_y, accel_z, steer]\")\n",
    "        print(f\"目标值尺寸: {target.shape}\")\n",
    "        print(f\"Loss: {loss.item():.4f}\\n\")\n",
    "\n",
    "        # 模拟展示如何将预测结果通过你写的反归一化还原\n",
    "        pred_numpy = predictions.detach().numpy()[0]\n",
    "        # 当你要获取真实的加速度和转角大小时，调用你的 inverse_transform 函数：\n",
    "        # real_values = inverse_transform(pred_numpy, target_names, scaler_params)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb282130ab9c02a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==================== 1. 视觉特征提取骨干 (ResNet + CBAM) ====================\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    \"\"\"通道注意力模块\"\"\"\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, in_planes // 16, 1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_planes // 16, in_planes, 1, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    \"\"\"空间注意力模块\"\"\"\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size // 2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    \"\"\"ResNet50的瓶颈块，集成CBAM注意力机制\"\"\"\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.ca = ChannelAttention(planes * 4)\n",
    "        self.sa = SpatialAttention()\n",
    "\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        out = self.ca(out) * out\n",
    "        out = self.sa(out) * out\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    \"\"\"完整的ResNet网络结构，集成CBAM注意力机制\"\"\"\n",
    "    def __init__(self, block, layers):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def get_feature_maps(self, x):\n",
    "        \"\"\"获取特征图（不进行全局池化），输出维度 (B, 2048, H/32, W/32)\"\"\"\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        return x\n",
    "\n",
    "def create_resnet_cbam():\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
    "\n",
    "# ==================== 2. 时序与状态处理模块 ====================\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"标准的正弦/余弦位置编码，适用于任意长度序列\"\"\"\n",
    "    def __init__(self, d_model, max_len=500):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0)) # Shape: (1, max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"x shape: (Batch, Seq_Len, d_model)\"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "class MotionLSTMEncoder(nn.Module):\n",
    "    \"\"\"处理车辆历史状态序列的 LSTM 模块\"\"\"\n",
    "    def __init__(self, input_size=3, hidden_size=512, num_layers=2):\n",
    "        super(MotionLSTMEncoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        self.ln = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"x shape: (Batch, N, 3)\"\"\"\n",
    "        output, (h_n, c_n) = self.lstm(x)\n",
    "        # 取最后一层的最后一个时间步状态\n",
    "        last_hidden = h_n[-1]\n",
    "        return self.ln(last_hidden)\n",
    "\n",
    "# ==================== 3. 核心双流融合模型 ====================\n",
    "\n",
    "class DualStreamDrivingModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 use_shared_weights=True,\n",
    "                 visual_d_model=512,\n",
    "                 lstm_hidden_size=512):\n",
    "        super(DualStreamDrivingModel, self).__init__()\n",
    "\n",
    "        self.use_shared_weights = use_shared_weights\n",
    "        self.visual_d_model = visual_d_model\n",
    "\n",
    "        # --- 1. 视觉流 (Visual Stream) ---\n",
    "        if use_shared_weights:\n",
    "            self.resnet = create_resnet_cbam()\n",
    "        else:\n",
    "            self.resnet_t_minus_2 = create_resnet_cbam()\n",
    "            self.resnet_t_minus_1 = create_resnet_cbam()\n",
    "            self.resnet_t = create_resnet_cbam()\n",
    "\n",
    "        self.feature_proj = nn.Linear(2048, visual_d_model)\n",
    "        self.pos_encoder = PositionalEncoding(visual_d_model, max_len=500)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=visual_d_model, nhead=8, batch_first=True, dim_feedforward=1024, dropout=0.1\n",
    "        )\n",
    "        self.visual_transformer = nn.TransformerEncoder(encoder_layer, num_layers=3)\n",
    "\n",
    "        # --- 2. 状态流 (State Stream) ---\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.motion_lstm = MotionLSTMEncoder(\n",
    "            input_size=3, # (v, a, angle)\n",
    "            hidden_size=lstm_hidden_size,\n",
    "            num_layers=2\n",
    "        )\n",
    "\n",
    "        # --- 3. 融合模块 (Fusion) ---\n",
    "        fusion_input_dim = visual_d_model + lstm_hidden_size\n",
    "        self.fusion_mlp = nn.Sequential(\n",
    "            nn.Linear(fusion_input_dim, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, lstm_hidden_size), # 输出必须对齐 LSTM 维度以进行残差相加\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # --- 4. 预测头 (Prediction Head) ---\n",
    "        self.control_head = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 2) # [加速度, 转角]\n",
    "        )\n",
    "\n",
    "    def extract_image_features(self, img_t_minus_2, img_t_minus_1, img_t):\n",
    "        if self.use_shared_weights:\n",
    "            f1 = self.resnet.get_feature_maps(img_t_minus_2)\n",
    "            f2 = self.resnet.get_feature_maps(img_t_minus_1)\n",
    "            f3 = self.resnet.get_feature_maps(img_t)\n",
    "        else:\n",
    "            f1 = self.resnet_t_minus_2.get_feature_maps(img_t_minus_2)\n",
    "            f2 = self.resnet_t_minus_1.get_feature_maps(img_t_minus_1)\n",
    "            f3 = self.resnet_t.get_feature_maps(img_t)\n",
    "        return f1, f2, f3\n",
    "\n",
    "    def forward(self, img_t_minus_2, img_t_minus_1, img_t, state_history):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_t_minus_2, img_t_minus_1, img_t: (Batch, 3, H, W)\n",
    "            state_history: (Batch, N, 3) 包含过去 N 帧的 [速度, 加速度, 转角]\n",
    "        \"\"\"\n",
    "        batch_size = img_t.size(0)\n",
    "\n",
    "        # ---------------- A. 视觉处理分支 ----------------\n",
    "        f1, f2, f3 = self.extract_image_features(img_t_minus_2, img_t_minus_1, img_t)\n",
    "\n",
    "        # 展平并投影\n",
    "        def flatten_and_project(f):\n",
    "            # f shape: (B, 2048, H', W') -> (B, H'*W', 2048)\n",
    "            flat = f.view(batch_size, f.size(1), -1).transpose(1, 2)\n",
    "            return self.feature_proj(flat)\n",
    "\n",
    "        proj_1 = flatten_and_project(f1) # (B, 49, d_model) 假设输入224x224\n",
    "        proj_2 = flatten_and_project(f2)\n",
    "        proj_3 = flatten_and_project(f3)\n",
    "\n",
    "        # 拼接序列，加入位置编码，送入 Transformer\n",
    "        visual_seq = torch.cat([proj_1, proj_2, proj_3], dim=1) # (B, 147, d_model)\n",
    "        visual_seq = self.pos_encoder(visual_seq)\n",
    "        trans_out = self.visual_transformer(visual_seq)\n",
    "\n",
    "        # 聚合视觉特征 (Global Average Pooling)\n",
    "        visual_vector = trans_out.mean(dim=1) # (Batch, visual_d_model)\n",
    "\n",
    "        # ---------------- B. 状态处理分支 ----------------\n",
    "        lstm_vector = self.motion_lstm(state_history) # (Batch, lstm_hidden_size)\n",
    "\n",
    "        # ---------------- C. 核心融合逻辑 ----------------\n",
    "        # 逻辑: Fused = MLP(Concat(Visual, LSTM)) + LSTM\n",
    "        combined_features = torch.cat([visual_vector, lstm_vector], dim=1)\n",
    "        mlp_output = self.fusion_mlp(combined_features)\n",
    "\n",
    "        # 残差连接 (Residual Add)\n",
    "        fused_final = mlp_output + lstm_vector\n",
    "\n",
    "        # ---------------- D. 预测输出 ----------------\n",
    "        prediction = self.control_head(fused_final) # (Batch, 2)\n",
    "\n",
    "        return prediction, {\n",
    "            'visual_vector': visual_vector,\n",
    "            'lstm_vector': lstm_vector,\n",
    "            'fused_final': fused_final\n",
    "        }\n",
    "\n",
    "\n",
    "# ==================== 测试用例 (Main) ====================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== 开始测试：双流端到端自动驾驶网络 ===\\n\")\n",
    "\n",
    "    # 1. 设定超参数\n",
    "    BATCH_SIZE = 2\n",
    "    HISTORY_N = 10     # 状态流输入过去 10 帧数据\n",
    "    IMG_SIZE = 224     # 图像分辨率\n",
    "\n",
    "    print(f\"设定参数: Batch Size = {BATCH_SIZE}, 历史帧数 = {HISTORY_N}, 图像尺寸 = {IMG_SIZE}x{IMG_SIZE}\")\n",
    "\n",
    "    # 2. 构造模拟的 Tensor 数据\n",
    "    # 图像数据: (B, C, H, W)\n",
    "    img_t2 = torch.randn(BATCH_SIZE, 3, IMG_SIZE, IMG_SIZE)\n",
    "    img_t1 = torch.randn(BATCH_SIZE, 3, IMG_SIZE, IMG_SIZE)\n",
    "    img_t0 = torch.randn(BATCH_SIZE, 3, IMG_SIZE, IMG_SIZE)\n",
    "\n",
    "    # 状态历史数据: (Batch, N, 3) 对应 [速度, 加速度, 转角]\n",
    "    state_seq = torch.randn(BATCH_SIZE, HISTORY_N, 3)\n",
    "\n",
    "    print(\"生成模拟数据完毕...\\n\")\n",
    "\n",
    "    # 3. 实例化模型\n",
    "    # visual_d_model 和 lstm_hidden_size 保持一致，方便做残差\n",
    "    model = DualStreamDrivingModel(\n",
    "        use_shared_weights=True,\n",
    "        visual_d_model=512,\n",
    "        lstm_hidden_size=512\n",
    "    )\n",
    "\n",
    "    print(\"模型实例化成功，开始前向传播 (Forward Pass)... (可能需要几秒钟计算)\")\n",
    "\n",
    "    # 4. 执行前向传播\n",
    "    model.eval() # 测试模式\n",
    "    with torch.no_grad():\n",
    "        predictions, debug_info = model(img_t2, img_t1, img_t0, state_seq)\n",
    "\n",
    "    # 5. 打印验证结果\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"内部特征维度检查:\")\n",
    "    print(f\" - Transformer 视觉向量: {debug_info['visual_vector'].shape} -> 期望: ({BATCH_SIZE}, 512)\")\n",
    "    print(f\" - LSTM 运动状态向量:    {debug_info['lstm_vector'].shape} -> 期望: ({BATCH_SIZE}, 512)\")\n",
    "    print(f\" - 残差融合后特征向量:   {debug_info['fused_final'].shape} -> 期望: ({BATCH_SIZE}, 512)\")\n",
    "\n",
    "    print(\"\\n模型最终预测输出:\")\n",
    "    print(f\" - 预测 Tensor 尺寸: {predictions.shape} -> 期望: ({BATCH_SIZE}, 2) [表示加速度和转角]\")\n",
    "\n",
    "    for i in range(BATCH_SIZE):\n",
    "        accel = predictions[i, 0].item()\n",
    "        steer = predictions[i, 1].item()\n",
    "        print(f\"   * 样本 {i+1}: 预测加速度 = {accel:+.4f}, 预测转角 = {steer:+.4f}\")\n",
    "\n",
    "    print(\"=\"*40)\n",
    "    print(\"测试通过！网络结构和数据流转完全正确。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
